{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>16.59</td>\n",
       "      <td>12154.99</td>\n",
       "      <td>110.25</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>18.43</td>\n",
       "      <td>16255.03</td>\n",
       "      <td>127.50</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>2.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>17.48</td>\n",
       "      <td>9208.34</td>\n",
       "      <td>95.96</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>18.16</td>\n",
       "      <td>15025.66</td>\n",
       "      <td>122.58</td>\n",
       "      <td>-0.63</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Model   MAE      MSE   RMSE    R2  MAPE\n",
       "0      Random Forest 16.59 12154.99 110.25 -0.32  2.59\n",
       "1      Decision Tree 18.43 16255.03 127.50 -0.77  2.27\n",
       "2  Linear Regression 17.48  9208.34  95.96 -0.00  3.68\n",
       "3            XGBoost 18.16 15025.66 122.58 -0.63  2.70"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Load your dataset (update the path accordingly if needed)\n",
    "file_path = 'budjet.xlsx'\n",
    "budjet_data = pd.read_excel(file_path, sheet_name='budjet')\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "# Convert the date into useful features\n",
    "budjet_data['date'] = pd.to_datetime(budjet_data['date'])\n",
    "budjet_data['year'] = budjet_data['date'].dt.year\n",
    "budjet_data['month'] = budjet_data['date'].dt.month\n",
    "budjet_data['day'] = budjet_data['date'].dt.day\n",
    "budjet_data['day_of_week'] = budjet_data['date'].dt.dayofweek\n",
    "\n",
    "# Encode the 'category' column using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "budjet_data['category_encoded'] = label_encoder.fit_transform(budjet_data['category'])\n",
    "\n",
    "# Drop the original 'date' and 'category' columns\n",
    "budjet_data_cleaned = budjet_data.drop(columns=['date', 'category'])\n",
    "\n",
    "# Step 2: Splitting the Data into Training and Testing Sets\n",
    "X = budjet_data_cleaned.drop(columns=['amount'])\n",
    "y = budjet_data_cleaned['amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define a dictionary of models to evaluate\n",
    "models_dict = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Step 4: Function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Step 5: Train and evaluate each model, collecting all metrics\n",
    "metrics_results = []\n",
    "\n",
    "for model_name, model in models_dict.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    # Append results to the list as a dictionary\n",
    "    metrics_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": metrics[\"MAE\"],\n",
    "        \"MSE\": metrics[\"MSE\"],\n",
    "        \"RMSE\": metrics[\"RMSE\"],\n",
    "        \"R2\": metrics[\"R2\"],\n",
    "        \"MAPE\": metrics[\"MAPE\"]\n",
    "    })\n",
    "\n",
    "# Step 6: Convert the results list into a DataFrame\n",
    "metrics_results_df = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Step 7: Display the results\n",
    "metrics_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 8, 'min_samples_split': 16, 'n_estimators': 116}\n",
      "Best parameters for Decision Tree: {'max_depth': 8, 'min_samples_split': 12}\n",
      "Best parameters for XGBoost: {'learning_rate': np.float64(0.1009124836035503), 'max_depth': 2, 'n_estimators': 67}\n",
      "               Model   MAE      MSE   RMSE    R2  MAPE\n",
      "0      Random Forest 13.76  8796.42  93.79  0.04  2.03\n",
      "1      Decision Tree 16.08 10863.09 104.23 -0.18  2.32\n",
      "2            XGBoost 14.68  8694.74  93.25  0.06  2.58\n",
      "3  Linear Regression 17.48  9208.34  95.96 -0.00  3.68\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Load your dataset (update the path accordingly if needed)\n",
    "file_path = 'budjet.xlsx'  # replace with your actual file path\n",
    "budjet_data = pd.read_excel(file_path, sheet_name='budjet')\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "budjet_data['date'] = pd.to_datetime(budjet_data['date'])\n",
    "budjet_data['year'] = budjet_data['date'].dt.year\n",
    "budjet_data['month'] = budjet_data['date'].dt.month\n",
    "budjet_data['day'] = budjet_data['date'].dt.day\n",
    "budjet_data['day_of_week'] = budjet_data['date'].dt.dayofweek\n",
    "\n",
    "# Encode the 'category' column using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "budjet_data['category_encoded'] = label_encoder.fit_transform(budjet_data['category'])\n",
    "\n",
    "# Drop the original 'date' and 'category' columns\n",
    "budjet_data_cleaned = budjet_data.drop(columns=['date', 'category'])\n",
    "\n",
    "# Step 2: Splitting the Data into Training and Testing Sets\n",
    "X = budjet_data_cleaned.drop(columns=['amount'])\n",
    "y = budjet_data_cleaned['amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define a dictionary of models to evaluate and their hyperparameter grids\n",
    "\n",
    "param_distributions = {\n",
    "    \"Random Forest\": {\n",
    "        'n_estimators': randint(10, 200),\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'max_depth': randint(2, 20),\n",
    "        'min_samples_split': randint(2, 20),\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        'n_estimators': randint(10, 200),\n",
    "        'learning_rate': uniform(0.01, 0.5),\n",
    "        'max_depth': randint(2, 20),\n",
    "    }\n",
    "}\n",
    "\n",
    "models_dict = {\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"XGBoost\": XGBRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Linear Regression has no hyperparameters to tune\n",
    "models_dict[\"Linear Regression\"] = LinearRegression()\n",
    "\n",
    "# Step 4: Function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Step 5: Hyperparameter tuning using RandomizedSearchCV\n",
    "best_models = {}\n",
    "for model_name, model in models_dict.items():\n",
    "    if model_name in param_distributions:\n",
    "        # Perform RandomizedSearchCV for models with hyperparameters\n",
    "        random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=20, cv=5, scoring='neg_mean_absolute_error', random_state=42, n_jobs=-1)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        best_models[model_name] = random_search.best_estimator_\n",
    "        print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
    "    else:\n",
    "        # No hyperparameters to tune for Linear Regression\n",
    "        model.fit(X_train, y_train)\n",
    "        best_models[model_name] = model\n",
    "\n",
    "# Step 6: Train and evaluate each model, collecting all metrics\n",
    "metrics_results = []\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    metrics = calculate_metrics(y_test, y_pred)\n",
    "    \n",
    "    # Append results to the list as a dictionary\n",
    "    metrics_results.append({\n",
    "        \"Model\": model_name,\n",
    "        \"MAE\": metrics[\"MAE\"],\n",
    "        \"MSE\": metrics[\"MSE\"],\n",
    "        \"RMSE\": metrics[\"RMSE\"],\n",
    "        \"R2\": metrics[\"R2\"],\n",
    "        \"MAPE\": metrics[\"MAPE\"]\n",
    "    })\n",
    "\n",
    "# Step 7: Convert the results list into a DataFrame\n",
    "metrics_results_df = pd.DataFrame(metrics_results)\n",
    "\n",
    "# Step 8: Display the results\n",
    "print(metrics_results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 8, 'min_samples_split': 20, 'n_estimators': 150}\n",
      "Best parameters for XGBoost: {'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 50}\n",
      "Random Forest Performance: {'MAE': np.float64(13.765277950080094), 'MSE': np.float64(8687.114808753782), 'RMSE': np.float64(93.2046930618506), 'R2': 0.05612479232435996, 'MAPE': np.float64(2.011194809329284)}\n",
      "XGBoost Performance: {'MAE': np.float64(15.256947376087432), 'MSE': np.float64(10316.201995660605), 'RMSE': np.float64(101.56870578904018), 'R2': -0.12087931556586207, 'MAPE': np.float64(2.287674207806325)}\n",
      "           Model   MAE      MSE   RMSE    R2  MAPE\n",
      "0  Random Forest 13.77  8687.11  93.20  0.06  2.01\n",
      "1        XGBoost 15.26 10316.20 101.57 -0.12  2.29\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "# Load your dataset (update the path accordingly if needed)\n",
    "file_path = 'budjet.xlsx'  # replace with your actual file path\n",
    "budjet_data = pd.read_excel(file_path, sheet_name='budjet')\n",
    "\n",
    "# Step 1: Data Preprocessing\n",
    "budjet_data['date'] = pd.to_datetime(budjet_data['date'])\n",
    "budjet_data['year'] = budjet_data['date'].dt.year\n",
    "budjet_data['month'] = budjet_data['date'].dt.month\n",
    "budjet_data['day'] = budjet_data['date'].dt.day\n",
    "budjet_data['day_of_week'] = budjet_data['date'].dt.dayofweek\n",
    "\n",
    "# Encode the 'category' column using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "budjet_data['category_encoded'] = label_encoder.fit_transform(budjet_data['category'])\n",
    "\n",
    "# Drop the original 'date' and 'category' columns\n",
    "budjet_data_cleaned = budjet_data.drop(columns=['date', 'category'])\n",
    "\n",
    "# Step 2: Splitting the Data into Training and Testing Sets\n",
    "X = budjet_data_cleaned.drop(columns=['amount'])\n",
    "y = budjet_data_cleaned['amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define parameter grids for Random Forest and XGBoost\n",
    "\n",
    "# Random Forest hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'min_samples_split': [10, 16, 20],\n",
    "}\n",
    "\n",
    "# XGBoost hyperparameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 67, 100],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [2, 4, 6]\n",
    "}\n",
    "\n",
    "# Step 4: GridSearchCV for hyperparameter tuning\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "best_rf_model = rf_grid_search.best_estimator_\n",
    "print(f\"Best parameters for Random Forest: {rf_grid_search.best_params_}\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "xgb_grid_search = GridSearchCV(xgb_model, xgb_param_grid, cv=5, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "xgb_grid_search.fit(X_train, y_train)\n",
    "best_xgb_model = xgb_grid_search.best_estimator_\n",
    "print(f\"Best parameters for XGBoost: {xgb_grid_search.best_params_}\")\n",
    "\n",
    "# Step 5: Define a function to calculate all metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    metrics = {\n",
    "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
    "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
    "        \"RMSE\": np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        \"R2\": r2_score(y_true, y_pred),\n",
    "        \"MAPE\": mean_absolute_percentage_error(y_true, y_pred)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# Step 6: Evaluate the optimized models\n",
    "\n",
    "# Predict with Random Forest\n",
    "rf_y_pred = best_rf_model.predict(X_test)\n",
    "rf_metrics = calculate_metrics(y_test, rf_y_pred)\n",
    "print(f\"Random Forest Performance: {rf_metrics}\")\n",
    "\n",
    "# Predict with XGBoost\n",
    "xgb_y_pred = best_xgb_model.predict(X_test)\n",
    "xgb_metrics = calculate_metrics(y_test, xgb_y_pred)\n",
    "print(f\"XGBoost Performance: {xgb_metrics}\")\n",
    "\n",
    "# Step 7: Combine the results into a DataFrame\n",
    "metrics_results = pd.DataFrame([\n",
    "    {\"Model\": \"Random Forest\", **rf_metrics},\n",
    "    {\"Model\": \"XGBoost\", **xgb_metrics}\n",
    "])\n",
    "\n",
    "# Step 8: Display the results\n",
    "print(metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-30 05:53:47,270] A new study created in memory with name: no-name-ec6df1cc-bc9d-4874-8a00-91e99f1aa687\n",
      "[I 2024-09-30 05:53:47,511] Trial 0 finished with value: 13.620032221464353 and parameters: {'n_estimators': 174, 'max_depth': 8, 'min_samples_split': 11}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:47,675] Trial 1 finished with value: 14.400216269212077 and parameters: {'n_estimators': 145, 'max_depth': 6, 'min_samples_split': 30}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:47,882] Trial 2 finished with value: 13.782119775173653 and parameters: {'n_estimators': 155, 'max_depth': 8, 'min_samples_split': 24}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:48,081] Trial 3 finished with value: 14.043128614189786 and parameters: {'n_estimators': 160, 'max_depth': 7, 'min_samples_split': 24}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:48,213] Trial 4 finished with value: 14.071817796064346 and parameters: {'n_estimators': 106, 'max_depth': 7, 'min_samples_split': 22}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:48,430] Trial 5 finished with value: 13.672404224662236 and parameters: {'n_estimators': 123, 'max_depth': 12, 'min_samples_split': 10}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:48,707] Trial 6 finished with value: 13.633107509222372 and parameters: {'n_estimators': 182, 'max_depth': 10, 'min_samples_split': 22}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:48,901] Trial 7 finished with value: 13.70233923041861 and parameters: {'n_estimators': 141, 'max_depth': 8, 'min_samples_split': 26}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:49,064] Trial 8 finished with value: 14.375431005244867 and parameters: {'n_estimators': 138, 'max_depth': 6, 'min_samples_split': 30}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:49,244] Trial 9 finished with value: 13.986552378217878 and parameters: {'n_estimators': 139, 'max_depth': 7, 'min_samples_split': 30}. Best is trial 0 with value: 13.620032221464353.\n",
      "[I 2024-09-30 05:53:49,542] Trial 10 finished with value: 13.525164895553036 and parameters: {'n_estimators': 179, 'max_depth': 10, 'min_samples_split': 13}. Best is trial 10 with value: 13.525164895553036.\n",
      "[I 2024-09-30 05:53:49,881] Trial 11 finished with value: 13.456319428797128 and parameters: {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 12}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:50,218] Trial 12 finished with value: 13.614023430183119 and parameters: {'n_estimators': 194, 'max_depth': 10, 'min_samples_split': 15}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:50,555] Trial 13 finished with value: 13.677399927034674 and parameters: {'n_estimators': 198, 'max_depth': 11, 'min_samples_split': 16}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:50,840] Trial 14 finished with value: 13.5995200076913 and parameters: {'n_estimators': 177, 'max_depth': 10, 'min_samples_split': 15}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:51,158] Trial 15 finished with value: 13.715665870028385 and parameters: {'n_estimators': 186, 'max_depth': 12, 'min_samples_split': 18}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:51,407] Trial 16 finished with value: 13.541558440281511 and parameters: {'n_estimators': 163, 'max_depth': 9, 'min_samples_split': 13}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:51,699] Trial 17 finished with value: 13.523442497652287 and parameters: {'n_estimators': 169, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:51,983] Trial 18 finished with value: 13.696705274209199 and parameters: {'n_estimators': 169, 'max_depth': 11, 'min_samples_split': 18}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:52,334] Trial 19 finished with value: 13.489569196495284 and parameters: {'n_estimators': 200, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:52,636] Trial 20 finished with value: 13.687966443683377 and parameters: {'n_estimators': 199, 'max_depth': 9, 'min_samples_split': 18}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:52,972] Trial 21 finished with value: 13.49806412131072 and parameters: {'n_estimators': 190, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:53,306] Trial 22 finished with value: 13.715346192229134 and parameters: {'n_estimators': 187, 'max_depth': 11, 'min_samples_split': 10}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:53,647] Trial 23 finished with value: 13.600220186526297 and parameters: {'n_estimators': 190, 'max_depth': 12, 'min_samples_split': 14}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:53,998] Trial 24 finished with value: 13.495192781377835 and parameters: {'n_estimators': 199, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:54,322] Trial 25 finished with value: 13.6492165078681 and parameters: {'n_estimators': 199, 'max_depth': 10, 'min_samples_split': 16}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:54,689] Trial 26 finished with value: 13.70162027889302 and parameters: {'n_estimators': 200, 'max_depth': 12, 'min_samples_split': 10}. Best is trial 11 with value: 13.456319428797128.\n",
      "[I 2024-09-30 05:53:54,882] Trial 27 finished with value: 13.43977647381494 and parameters: {'n_estimators': 123, 'max_depth': 9, 'min_samples_split': 12}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:55,074] Trial 28 finished with value: 13.696577763627259 and parameters: {'n_estimators': 125, 'max_depth': 9, 'min_samples_split': 17}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:55,241] Trial 29 finished with value: 13.619314116565452 and parameters: {'n_estimators': 109, 'max_depth': 9, 'min_samples_split': 20}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:55,430] Trial 30 finished with value: 13.608896626602457 and parameters: {'n_estimators': 127, 'max_depth': 8, 'min_samples_split': 14}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:55,623] Trial 31 finished with value: 13.567397592033732 and parameters: {'n_estimators': 115, 'max_depth': 10, 'min_samples_split': 12}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:55,923] Trial 32 finished with value: 13.60369115013417 and parameters: {'n_estimators': 172, 'max_depth': 11, 'min_samples_split': 11}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:56,090] Trial 33 finished with value: 13.450174883123426 and parameters: {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 13}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:56,253] Trial 34 finished with value: 13.498370606988793 and parameters: {'n_estimators': 101, 'max_depth': 9, 'min_samples_split': 14}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:56,453] Trial 35 finished with value: 13.615140333783636 and parameters: {'n_estimators': 116, 'max_depth': 10, 'min_samples_split': 11}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:56,673] Trial 36 finished with value: 13.619877035147136 and parameters: {'n_estimators': 149, 'max_depth': 8, 'min_samples_split': 13}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:56,895] Trial 37 finished with value: 13.602080188933218 and parameters: {'n_estimators': 131, 'max_depth': 10, 'min_samples_split': 10}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:57,053] Trial 38 finished with value: 13.530606428282779 and parameters: {'n_estimators': 101, 'max_depth': 9, 'min_samples_split': 15}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:57,254] Trial 39 finished with value: 13.589542216251338 and parameters: {'n_estimators': 117, 'max_depth': 9, 'min_samples_split': 11}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:57,517] Trial 40 finished with value: 13.66162601120968 and parameters: {'n_estimators': 158, 'max_depth': 10, 'min_samples_split': 21}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:57,857] Trial 41 finished with value: 13.521215862304444 and parameters: {'n_estimators': 194, 'max_depth': 11, 'min_samples_split': 12}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:58,089] Trial 42 finished with value: 13.526513101429428 and parameters: {'n_estimators': 133, 'max_depth': 11, 'min_samples_split': 13}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:58,275] Trial 43 finished with value: 13.615918820359667 and parameters: {'n_estimators': 111, 'max_depth': 12, 'min_samples_split': 26}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:58,515] Trial 44 finished with value: 13.498905739091482 and parameters: {'n_estimators': 145, 'max_depth': 10, 'min_samples_split': 12}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:58,826] Trial 45 finished with value: 13.601324700177614 and parameters: {'n_estimators': 182, 'max_depth': 11, 'min_samples_split': 14}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:59,124] Trial 46 finished with value: 13.56769856571056 and parameters: {'n_estimators': 192, 'max_depth': 10, 'min_samples_split': 28}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:59,437] Trial 47 finished with value: 13.66851695521413 and parameters: {'n_estimators': 185, 'max_depth': 11, 'min_samples_split': 16}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:59,668] Trial 48 finished with value: 14.225094518475023 and parameters: {'n_estimators': 195, 'max_depth': 6, 'min_samples_split': 11}. Best is trial 27 with value: 13.43977647381494.\n",
      "[I 2024-09-30 05:53:59,845] Trial 49 finished with value: 13.54948315404434 and parameters: {'n_estimators': 121, 'max_depth': 8, 'min_samples_split': 13}. Best is trial 27 with value: 13.43977647381494.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 123, 'max_depth': 9, 'min_samples_split': 12}\n",
      "Optimized Random Forest Performance: {'MAE': np.float64(13.43977647381494), 'MSE': np.float64(8994.027751377778), 'RMSE': np.float64(94.83684806749842), 'R2': 0.022777988024541673, 'MAPE': np.float64(1.9690135279525145)}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 3: Load your dataset (update the path accordingly if needed)\n",
    "file_path = 'budjet.xlsx'  # replace with your actual file path\n",
    "budjet_data = pd.read_excel(file_path, sheet_name='budjet')\n",
    "\n",
    "# Step 4: Data Preprocessing\n",
    "budjet_data['date'] = pd.to_datetime(budjet_data['date'])\n",
    "budjet_data['year'] = budjet_data['date'].dt.year\n",
    "budjet_data['month'] = budjet_data['date'].dt.month\n",
    "budjet_data['day'] = budjet_data['date'].dt.day\n",
    "budjet_data['day_of_week'] = budjet_data['date'].dt.dayofweek\n",
    "\n",
    "# Encode the 'category' column using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "budjet_data['category_encoded'] = label_encoder.fit_transform(budjet_data['category'])\n",
    "\n",
    "# Drop the original 'date' and 'category' columns\n",
    "budjet_data_cleaned = budjet_data.drop(columns=['date', 'category'])\n",
    "\n",
    "# Step 5: Split the Data\n",
    "X = budjet_data_cleaned.drop(columns=['amount'])\n",
    "y = budjet_data_cleaned['amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 10, 30)\n",
    "\n",
    "    # Create the RandomForestRegressor with trial parameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# Step 7: Optimize the objective function using Optuna\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize MAE\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Step 8: Display the best parameters and performance\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 9: Train the model with the best hyperparameters\n",
    "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 10: Evaluate the optimized model\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# Step 11: Display the results\n",
    "metrics_results = {\n",
    "    \"MAE\": mae,\n",
    "    \"MSE\": mse,\n",
    "    \"RMSE\": rmse,\n",
    "    \"R2\": r2,\n",
    "    \"MAPE\": mape\n",
    "}\n",
    "print(\"Optimized Random Forest Performance:\", metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-30 05:58:38,402] A new study created in memory with name: no-name-f0baf494-f4b4-46f0-969b-f07a11deca41\n",
      "[I 2024-09-30 05:58:38,613] Trial 0 finished with value: 3.086975700577281 and parameters: {'n_estimators': 166, 'max_depth': 8, 'min_samples_split': 14}. Best is trial 0 with value: 3.086975700577281.\n",
      "[I 2024-09-30 05:58:38,842] Trial 1 finished with value: 3.089592773585585 and parameters: {'n_estimators': 198, 'max_depth': 7, 'min_samples_split': 20}. Best is trial 0 with value: 3.086975700577281.\n",
      "[I 2024-09-30 05:58:39,083] Trial 2 finished with value: 3.097057585355076 and parameters: {'n_estimators': 199, 'max_depth': 7, 'min_samples_split': 10}. Best is trial 0 with value: 3.086975700577281.\n",
      "[I 2024-09-30 05:58:39,245] Trial 3 finished with value: 3.0789685737738206 and parameters: {'n_estimators': 119, 'max_depth': 9, 'min_samples_split': 19}. Best is trial 3 with value: 3.0789685737738206.\n",
      "[I 2024-09-30 05:58:39,446] Trial 4 finished with value: 3.091398955756106 and parameters: {'n_estimators': 173, 'max_depth': 7, 'min_samples_split': 22}. Best is trial 3 with value: 3.0789685737738206.\n",
      "[I 2024-09-30 05:58:39,715] Trial 5 finished with value: 3.078006517822539 and parameters: {'n_estimators': 182, 'max_depth': 12, 'min_samples_split': 22}. Best is trial 5 with value: 3.078006517822539.\n",
      "[I 2024-09-30 05:58:39,915] Trial 6 finished with value: 3.0926213881819575 and parameters: {'n_estimators': 173, 'max_depth': 7, 'min_samples_split': 27}. Best is trial 5 with value: 3.078006517822539.\n",
      "[I 2024-09-30 05:58:40,152] Trial 7 finished with value: 3.081155614135025 and parameters: {'n_estimators': 179, 'max_depth': 9, 'min_samples_split': 22}. Best is trial 5 with value: 3.078006517822539.\n",
      "[I 2024-09-30 05:58:40,464] Trial 8 finished with value: 3.096454587151962 and parameters: {'n_estimators': 196, 'max_depth': 12, 'min_samples_split': 11}. Best is trial 5 with value: 3.078006517822539.\n",
      "[I 2024-09-30 05:58:40,709] Trial 9 finished with value: 3.0768213092548944 and parameters: {'n_estimators': 170, 'max_depth': 12, 'min_samples_split': 27}. Best is trial 9 with value: 3.0768213092548944.\n",
      "[I 2024-09-30 05:58:40,906] Trial 10 finished with value: 3.075352294079507 and parameters: {'n_estimators': 140, 'max_depth': 11, 'min_samples_split': 30}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:41,103] Trial 11 finished with value: 3.0762309154467538 and parameters: {'n_estimators': 138, 'max_depth': 11, 'min_samples_split': 30}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:41,293] Trial 12 finished with value: 3.0757531189858955 and parameters: {'n_estimators': 134, 'max_depth': 11, 'min_samples_split': 30}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:41,486] Trial 13 finished with value: 3.076823884929821 and parameters: {'n_estimators': 139, 'max_depth': 10, 'min_samples_split': 30}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:41,630] Trial 14 finished with value: 3.078204548368526 and parameters: {'n_estimators': 102, 'max_depth': 10, 'min_samples_split': 26}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:41,844] Trial 15 finished with value: 3.0764322827085766 and parameters: {'n_estimators': 148, 'max_depth': 11, 'min_samples_split': 25}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:42,029] Trial 16 finished with value: 3.080464959345399 and parameters: {'n_estimators': 123, 'max_depth': 11, 'min_samples_split': 17}. Best is trial 10 with value: 3.075352294079507.\n",
      "[I 2024-09-30 05:58:42,242] Trial 17 finished with value: 3.0736561168002194 and parameters: {'n_estimators': 155, 'max_depth': 10, 'min_samples_split': 30}. Best is trial 17 with value: 3.0736561168002194.\n",
      "[I 2024-09-30 05:58:42,461] Trial 18 finished with value: 3.0750467833188755 and parameters: {'n_estimators': 156, 'max_depth': 10, 'min_samples_split': 24}. Best is trial 17 with value: 3.0736561168002194.\n",
      "[I 2024-09-30 05:58:42,675] Trial 19 finished with value: 3.0812581594809347 and parameters: {'n_estimators': 158, 'max_depth': 9, 'min_samples_split': 24}. Best is trial 17 with value: 3.0736561168002194.\n",
      "[I 2024-09-30 05:58:42,887] Trial 20 finished with value: 3.0736429117725343 and parameters: {'n_estimators': 153, 'max_depth': 10, 'min_samples_split': 28}. Best is trial 20 with value: 3.0736429117725343.\n",
      "[I 2024-09-30 05:58:43,102] Trial 21 finished with value: 3.0724899564736066 and parameters: {'n_estimators': 156, 'max_depth': 10, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:43,311] Trial 22 finished with value: 3.0741547040071566 and parameters: {'n_estimators': 150, 'max_depth': 10, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:43,525] Trial 23 finished with value: 3.0786094186413777 and parameters: {'n_estimators': 161, 'max_depth': 9, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:43,709] Trial 24 finished with value: 3.0842018958708746 and parameters: {'n_estimators': 145, 'max_depth': 8, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:43,888] Trial 25 finished with value: 3.0750308750266253 and parameters: {'n_estimators': 128, 'max_depth': 10, 'min_samples_split': 26}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:44,083] Trial 26 finished with value: 3.0833595140706365 and parameters: {'n_estimators': 154, 'max_depth': 8, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:44,266] Trial 27 finished with value: 3.1033640130776416 and parameters: {'n_estimators': 163, 'max_depth': 6, 'min_samples_split': 24}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:44,462] Trial 28 finished with value: 3.079699808277146 and parameters: {'n_estimators': 145, 'max_depth': 9, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:44,732] Trial 29 finished with value: 3.0752789960057365 and parameters: {'n_estimators': 187, 'max_depth': 10, 'min_samples_split': 17}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:44,957] Trial 30 finished with value: 3.080288547733352 and parameters: {'n_estimators': 168, 'max_depth': 9, 'min_samples_split': 26}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:45,168] Trial 31 finished with value: 3.073289558450595 and parameters: {'n_estimators': 154, 'max_depth': 10, 'min_samples_split': 28}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:45,379] Trial 32 finished with value: 3.073012004509222 and parameters: {'n_estimators': 153, 'max_depth': 10, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:45,618] Trial 33 finished with value: 3.0745839568754985 and parameters: {'n_estimators': 164, 'max_depth': 11, 'min_samples_split': 27}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:45,839] Trial 34 finished with value: 3.08053656571226 and parameters: {'n_estimators': 151, 'max_depth': 10, 'min_samples_split': 13}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:46,033] Trial 35 finished with value: 3.082250252095834 and parameters: {'n_estimators': 143, 'max_depth': 9, 'min_samples_split': 25}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:46,177] Trial 36 finished with value: 3.083113099564552 and parameters: {'n_estimators': 114, 'max_depth': 8, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:46,399] Trial 37 finished with value: 3.0773499215821802 and parameters: {'n_estimators': 135, 'max_depth': 11, 'min_samples_split': 20}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:46,650] Trial 38 finished with value: 3.0746373170713124 and parameters: {'n_estimators': 177, 'max_depth': 10, 'min_samples_split': 22}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:46,867] Trial 39 finished with value: 3.080591304425858 and parameters: {'n_estimators': 160, 'max_depth': 9, 'min_samples_split': 23}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:47,049] Trial 40 finished with value: 3.07322853668977 and parameters: {'n_estimators': 130, 'max_depth': 10, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:47,235] Trial 41 finished with value: 3.0737783577818854 and parameters: {'n_estimators': 132, 'max_depth': 10, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:47,393] Trial 42 finished with value: 3.0756646585642855 and parameters: {'n_estimators': 112, 'max_depth': 10, 'min_samples_split': 27}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:47,609] Trial 43 finished with value: 3.0728900025356314 and parameters: {'n_estimators': 151, 'max_depth': 11, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:47,797] Trial 44 finished with value: 3.0763289540327388 and parameters: {'n_estimators': 129, 'max_depth': 12, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:48,038] Trial 45 finished with value: 3.0764227507501407 and parameters: {'n_estimators': 167, 'max_depth': 11, 'min_samples_split': 26}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:48,253] Trial 46 finished with value: 3.0755895473699 and parameters: {'n_estimators': 147, 'max_depth': 12, 'min_samples_split': 29}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:48,457] Trial 47 finished with value: 3.0755161995584124 and parameters: {'n_estimators': 141, 'max_depth': 11, 'min_samples_split': 27}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:48,709] Trial 48 finished with value: 3.0759504182871438 and parameters: {'n_estimators': 173, 'max_depth': 11, 'min_samples_split': 25}. Best is trial 21 with value: 3.0724899564736066.\n",
      "[I 2024-09-30 05:58:48,881] Trial 49 finished with value: 3.075610194897587 and parameters: {'n_estimators': 120, 'max_depth': 11, 'min_samples_split': 30}. Best is trial 21 with value: 3.0724899564736066.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 156, 'max_depth': 10, 'min_samples_split': 28}\n",
      "Optimized Random Forest Performance: {'MAE': np.float64(3.0724899564736066), 'MSE': np.float64(16.970983275680737), 'RMSE': np.float64(4.1195853281223265), 'R2': 0.16695339979719026, 'MAPE': np.float64(0.9401607904252441)}\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Import necessary libraries\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 3: Load your dataset (update the path accordingly if needed)\n",
    "file_path = 'budjet.xlsx'  # replace with your actual file path\n",
    "budjet_data = pd.read_excel(file_path, sheet_name='budjet')\n",
    "\n",
    "# Step 4: Data Preprocessing\n",
    "budjet_data['date'] = pd.to_datetime(budjet_data['date'])\n",
    "budjet_data['year'] = budjet_data['date'].dt.year\n",
    "budjet_data['month'] = budjet_data['date'].dt.month\n",
    "budjet_data['day'] = budjet_data['date'].dt.day\n",
    "budjet_data['day_of_week'] = budjet_data['date'].dt.dayofweek\n",
    "\n",
    "# Encode the 'category' column using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "budjet_data['category_encoded'] = label_encoder.fit_transform(budjet_data['category'])\n",
    "\n",
    "# Drop the original 'date' and 'category' columns\n",
    "budjet_data_cleaned = budjet_data.drop(columns=['date', 'category'])\n",
    "\n",
    "# Step 5: Remove Outliers Using IQR\n",
    "Q1 = budjet_data_cleaned.quantile(0.25)\n",
    "Q3 = budjet_data_cleaned.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Remove outliers outside 1.5*IQR range\n",
    "budjet_data_cleaned = budjet_data_cleaned[~((budjet_data_cleaned < (Q1 - 1.5 * IQR)) | (budjet_data_cleaned > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Step 6: Split the data into features (X) and target (y)\n",
    "X = budjet_data_cleaned.drop(columns=['amount'])\n",
    "y = budjet_data_cleaned['amount']\n",
    "\n",
    "# Step 7: Scale the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define the hyperparameter search space\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 6, 12)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 10, 30)\n",
    "\n",
    "    # Create the RandomForestRegressor with trial parameters\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate the MAE\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# Step 9: Optimize the objective function using Optuna\n",
    "study = optuna.create_study(direction='minimize')  # We want to minimize MAE\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Step 10: Display the best parameters and performance\n",
    "best_params = study.best_params\n",
    "print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "# Step 11: Train the model with the best hyperparameters\n",
    "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 12: Evaluate the optimized model\n",
    "y_pred = best_rf_model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "# Step 13: Display the results\n",
    "metrics_results = {\n",
    "    \"MAE\": mae,\n",
    "    \"MSE\": mse,\n",
    "    \"RMSE\": rmse,\n",
    "    \"R2\": r2,\n",
    "    \"MAPE\": mape\n",
    "}\n",
    "print(\"Optimized Random Forest Performance:\", metrics_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
